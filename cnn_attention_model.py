# -*- coding: utf-8 -*-
"""CSE424PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hNxYmXj8ve6q1TrD90tX4R55YoPnY5Ju

**Pretraining the model on CIFAR 100 dataset**
"""

import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import random_split

# Define transformations for CIFAR-100
cifar_transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((128, 128)),  # Resize CIFAR images to match your input size
    transforms.ToTensor(),       # Convert to PyTorch tensors
    transforms.Normalize((0.5,), (0.5,))  # Normalize images
])

# Load CIFAR-100 dataset
train_cifar100 = torchvision.datasets.CIFAR100(
    root='./data', train=True, download=True, transform=cifar_transform
)
test_cifar100 = torchvision.datasets.CIFAR100(
    root='./data', train=False, download=True, transform=cifar_transform
)


# Define the split ratio
train_size = int(0.8 * len(train_cifar100))  # 80% for training
val_size = len(train_cifar100) - train_size  # 20% for validation

# Split the dataset
train_cifar100, val_cifar100 = random_split(train_cifar100, [train_size, val_size])

# Create DataLoaders
train_loader_cifar100 = torch.utils.data.DataLoader(train_cifar100, batch_size=512, shuffle=True, num_workers=2)
val_loader_cifar100 = torch.utils.data.DataLoader(val_cifar100, batch_size=512, shuffle=False, num_workers=2)
test_loader_cifar100 = torch.utils.data.DataLoader(test_cifar100, batch_size=512, shuffle=False, num_workers=2)

import torch
import torch.nn as nn
import torch.nn.functional as F

class CustomCNN(nn.Module):
    def __init__(self, num_classes=100):  # CIFAR-100 has 100 classes
        super(CustomCNN, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Input: 128x128x1 -> Output: 128x128x32
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),  # Output: 128x128x32
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x64x32

            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: 64x64x64
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Output: 64x64x64
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 32x32x64

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: 32x32x128
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),  # Output: 32x32x128
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16x16x128

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Output: 16x16x256
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Output: 16x16x256
            nn.ReLU(),
        )

        self.fc = nn.Sequential(
            nn.Flatten(),  # Output: 16*16*256 = 65536
            nn.Linear(65536, 256),     #output of fc1 was told to be 65536 but it crashed the runtime, so we went with 256 for faster processing
            nn.ReLU(),
            nn.Linear(256, num_classes)  # Output: num_classes (100 for CIFAR-100)
        )

        # Apply Xavier Initialization
        self._initialize_weights()

    def forward(self, x):
        x = self.encoder(x)
        x = self.fc(x)
        return x

    def _initialize_weights(self):
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)  # Xavier initialization
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0)  # Initialize bias to 0

import torch.optim as optim
from tqdm import tqdm
import matplotlib.pyplot as plt

# Initialize the model, loss function, and optimizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CustomCNN(num_classes=100).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=0.001)

# Training and validation loop
num_epochs = 5
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    # Training phase
    model.train()
    running_train_loss = 0.0
    for images, labels in tqdm(train_loader_cifar100, desc=f"Training Epoch {epoch+1}/{num_epochs}"):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        train_loss = criterion(outputs, labels)
        train_loss.backward()
        optimizer.step()

        running_train_loss += train_loss.item()

    epoch_train_loss = running_train_loss / len(train_loader_cifar100)
    train_losses.append(epoch_train_loss)
    print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_train_loss:.4f}")

    # Validation phase
    model.eval()
    running_val_loss = 0.0
    with torch.no_grad():
        for val_images, val_labels in tqdm(val_loader_cifar100, desc=f"Validation Epoch {epoch+1}/{num_epochs}"):
            val_images, val_labels = val_images.to(device), val_labels.to(device)

            val_outputs = model(val_images)
            val_loss = criterion(val_outputs, val_labels)

            running_val_loss += val_loss.item()

    epoch_val_loss = running_val_loss / len(val_loader_cifar100)
    val_losses.append(epoch_val_loss)
    print(f"Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_val_loss:.4f}")

# Save the pretrained weights
torch.save(model.state_dict(), './cifar100_pretrained_weights.pth')
print("Pretrained weights saved!")

# Plot training and validation losses
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs+1), train_losses, label="Training Loss")
plt.plot(range(1, num_epochs+1), val_losses, label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training and Validation Loss")
plt.legend()
plt.grid()
plt.show()

from google.colab import files

# Save weights locally in Colab
torch.save(model.state_dict(), './cifar100_pretrained_weights.pth')
print("Pretrained weights saved!")

# Download the weights to your computer
#files.download('./cifar100_pretrained_weights.pth')

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

# Switch to evaluation mode
model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in test_loader_cifar100:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

import seaborn as sns
from sklearn.metrics import ConfusionMatrixDisplay

# Generate confusion matrix
cm = confusion_matrix(all_labels, all_preds)

# Plot confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=False, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()
# Print classification report
print(classification_report(all_labels, all_preds))

"""**Finetuning the model to MatriVassha Dataset**"""

from google.colab import drive
drive.mount('/content/drive')

zip_path = '/content/drive/My Drive/MatriVasha/MatriVasha_raw.zip'

import zipfile

# Destination folder to extract the dataset
extract_path = '/content/MatriVasha_raw/'

# Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Dataset extracted successfully!")

import os

# List the extracted files
print(os.listdir(extract_path))

# Check the structure of the extracted dataset
import os

root_dir = '/content/MatriVasha_raw/'
for folder in os.listdir(root_dir):
    print(folder, ":", len(os.listdir(os.path.join(root_dir, folder))), "items")

root_dir = '/content/MatriVasha_raw/MatriVasha_raw'
male_dir = os.path.join(root_dir, 'male')
female_dir = os.path.join(root_dir, 'feamale')  # Notice "feamale" instead of "female"

import os

# Check male directory contents
print("Male directory contents:")
if os.path.exists(male_dir):
    print(os.listdir(male_dir))
else:
    print("Male directory not found!")

# Check female directory contents
print("\nFemale directory contents:")
if os.path.exists(female_dir):
    print(os.listdir(female_dir))
else:
    print("Female directory not found!")

import shutil

# Merged output directory
merged_dir = '/content/MatriVasha_merged/'

# Create the merged directory if it doesn't exist
if not os.path.exists(merged_dir):
    os.makedirs(merged_dir)

for label in range(120):  # Loop through class folders (0-119)
    label_str = str(label)

    male_folder = os.path.join(male_dir, label_str)
    female_folder = os.path.join(female_dir, label_str)
    merged_folder = os.path.join(merged_dir, label_str)

    # Create the merged folder if it doesn't exist
    if not os.path.exists(merged_folder):
        os.makedirs(merged_folder)

    # Copy images from male folder
    if os.path.exists(male_folder):
        for file in os.listdir(male_folder):
            src = os.path.join(male_folder, file)
            dest = os.path.join(merged_folder, file)
            shutil.copy(src, dest)

    # Copy images from female folder
    if os.path.exists(female_folder):
        for file in os.listdir(female_folder):
            src = os.path.join(female_folder, file)
            dest = os.path.join(merged_folder, file)
            shutil.copy(src, dest)

print("Merging complete!")

for folder in os.listdir(merged_dir):
    folder_path = os.path.join(merged_dir, folder)
    print(f"Class {folder}: {len(os.listdir(folder_path))} images")

import os
import random
import matplotlib.pyplot as plt
from PIL import Image
from PIL import ImageOps

# Path to the merged dataset
merged_dir = '/content/MatriVasha_merged/'

# Get a list of all class folders
class_folders = os.listdir(merged_dir)

# Function to display 10 random images
def show_random_images_from_folder(merged_dir, num_images=10):
    plt.figure(figsize=(15, 5))

    for i in range(num_images):
        # Choose a random class folder
        random_class = random.choice(class_folders)
        class_folder_path = os.path.join(merged_dir, random_class)

        # Choose a random image from the chosen class folder
        random_image = random.choice(os.listdir(class_folder_path))
        image_path = os.path.join(class_folder_path, random_image)

        # Load and display the image
        img = Image.open(image_path).convert('L')  # Convert to grayscale
        #img = ImageOps.invert(img)                 # i have inverted the images, so that the lines are brighter, having more weight
        plt.subplot(1, num_images, i + 1)
        plt.imshow(img, cmap='gray')
        plt.title(f"Class: {random_class}")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Call the function
show_random_images_from_folder(merged_dir)

import os
from PIL import Image
from PIL import Image, ImageOps
from torch.utils.data import Dataset

class MatriVashaDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        """
        Args:
            root_dir (str): Path to the dataset directory (merged directory).
            transform (callable, optional): Transformations to apply to the images.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []

        # Load all image paths and their labels
        for label in os.listdir(root_dir):  # Class folders
            class_folder = os.path.join(root_dir, label)
            if os.path.isdir(class_folder):
                for img_file in os.listdir(class_folder):
                    self.image_paths.append(os.path.join(class_folder, img_file))
                    self.labels.append(int(label))  # Class label as integer

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]

        # Load image
        image = Image.open(img_path).convert('L')  # Convert to grayscale
        image = ImageOps.invert(image)            # Invert the grayscale image

        # Apply transformations if specified
        if self.transform:
            image = self.transform(image)

        return image, label

# Path to the merged dataset directory
merged_dir = '/content/MatriVasha_merged/'

# Define any image transformations
from torchvision.transforms import Compose, ToTensor, Normalize, Resize

transform = Compose([
    Resize((128, 128)),  # Resize to 128x128
    ToTensor(),        # Convert to PyTorch tensor
    Normalize((0.5,), (0.5,))  # Normalize to mean=0.5, std=0.5
])

# Create the dataset instance
dataset = MatriVashaDataset(root_dir=merged_dir, transform=transform)

from torch.utils.data import random_split

# Define the split proportions
total_size = len(dataset)
train_size = int(0.7 * total_size)  # 70% for training
val_size = int(0.1 * total_size)   # 10% for validation
test_size = total_size - train_size - val_size  # Remaining 20% for testing

# Perform the split
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# Print sizes of each split
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Testing samples: {len(test_dataset)}")

# Check the total number of samples
print(f"Total samples in dataset: {len(dataset)}")

# Check the first sample
image, label = dataset[6633]
print(f"First image shape: {image.shape}, Label: {label}")

from torch.utils.data import DataLoader

batch_size = 64

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=2)

import matplotlib.pyplot as plt

# Function to visualize one image from each class
def show_one_per_class(dataset, rows=12, cols=10): #Changed rows to 12 to accommodate 120 images
    class_images = {}  # Dictionary to store one image per class

    for img, label in dataset:
        label = int(label)  # Convert tensor to Python integer
        if label not in class_images:  # Add one image per class
            class_images[label] = img
        if len(class_images) == 120:  # Stop if all classes are covered
            break

    # Sort images by class label
    sorted_classes = sorted(class_images.keys())
    images = [class_images[label] for label in sorted_classes]

    # Plot the images
    fig, axes = plt.subplots(rows, cols, figsize=(20, 24)) #changed figsize to accommodate 12 rows
    for i, (img, label) in enumerate(zip(images, sorted_classes)):
        row, col = divmod(i, cols)
        img = img.squeeze(0).numpy()  # Remove channel dimension for grayscale
        axes[row, col].imshow(img, cmap='gray')
        axes[row, col].set_title(f"Class: {label}")
        axes[row, col].axis('off')

    plt.tight_layout()
    plt.show()

show_one_per_class(train_loader.dataset)

print(64*64)

import torch
import torch.nn as nn
import torch.nn.functional as F


class CNNEncoderWithAttention(nn.Module):
    def __init__(self, num_classes):
        super(CNNEncoderWithAttention, self).__init__()

        # Convolutional Layers
        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # B x 32 x 128 x 128
        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # B x 32 x 64 x 64

        self.conv2_1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)
        self.conv2_2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # B x 64 x 32 x 32

        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # B x 128 x 16 x 16

        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)

        # Attention Mechanisms
        self.mapOne = nn.ModuleList([nn.Linear(128 * 128, 256) for _ in range(32)])
        self.mapTwo = nn.ModuleList([nn.Linear(64 * 64, 256) for _ in range(64)])
        self.mapThree = nn.ModuleList([nn.Linear(32 * 32, 256) for _ in range(128)])

        # Fully connected layers
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(16 * 16 * 256, 256)
        self.fc2 = nn.Linear(57344, num_classes)

    def forward(self, x):
        # Convolutional Block 1
        x = F.relu(self.conv1_1(x))
        x = F.relu(self.conv1_2(x))
        l1 = x  # B x 32 x 128 x 128
        x = self.pool1(x)

        # Convolutional Block 2
        x = F.relu(self.conv2_1(x))
        x = F.relu(self.conv2_2(x))
        l2 = x  # B x 64 x 64 x 64
        x = self.pool2(x)

        # Convolutional Block 3
        x = F.relu(self.conv3_1(x))
        x = F.relu(self.conv3_2(x))
        l3 = x  # B x 128 x 32 x 32
        x = self.pool3(x)

        # Convolutional Block 4
        x = F.relu(self.conv4_1(x))
        x = F.relu(self.conv4_2(x))  # B x 256 x 16 x 16
        x = self.flatten(x)

        # Fully connected representation
        g = F.relu(self.fc1(x))  # B x 256
        g1 = g.unsqueeze(-1)  # B x 256 x 1

        # Attention Mechanisms with Heatmap Creation
        # Attention on l1
        l1 = l1.view(l1.size(0), l1.size(1), -1)  # B x 32 x (128 * 128)
        g_aOne, attention_map1 = self.attention_mechanism(l1, self.mapOne, g1, feature_shape=(128, 128))

        # Attention on l2
        l2 = l2.view(l2.size(0), l2.size(1), -1)  # B x 64 x (64 * 64)
        g_aTwo, attention_map2 = self.attention_mechanism(l2, self.mapTwo, g1, feature_shape=(64, 64))

        # Attention on l3
        l3 = l3.view(l3.size(0), l3.size(1), -1)  # B x 128 x (32 * 32)
        g_aThree, attention_map3 = self.attention_mechanism(l3, self.mapThree, g1, feature_shape=(32, 32))


        # Combine attention outputs
        combined_attention = torch.cat([g_aOne, g_aTwo, g_aThree], dim=1)
        #print(combined_attention.shape)
        x = combined_attention.flatten(1)
        #print("shape b4 fc2: ",x.shape)

        # Final classification
        x = self.fc2(x)  # Output logits

        return x, attention_map1, attention_map2, attention_map3


    def attention_mechanism(self, l, map_modules, g1, feature_shape):

        projections = []
        for i in range(l.size(1)):  # Iterate over channels
            proj = map_modules[i](l[:, i, :]).unsqueeze(1)  # B x 1 x 256
            projections.append(proj)

        projections = torch.cat(projections, dim=1)  # B x C x 256
        #print(projections.shape)
        attention_weights = F.softmax(torch.matmul(projections, g1), dim=1)  # B x C x 1
        #print(attention_weights.shape)

        # Attention-weighted features
        attended_features = (attention_weights * projections)  # B x C x 256
        #print(attended_features.shape)

        #to create attention map multiply each channel of projection with its respective channel of L1. The weighted channels will be BxCxHxW. Then attention map will be mean of the weighted channel across dim 1, of shape BxXxY

         # Reshape `l` to spatial dimensions (B x C x H x W)
        spatial_l = l.view(l.size(0), l.size(1), *feature_shape)  # B x C x H x W

        # Compute weighted channels (B x C x H x W)
        weighted_channels = spatial_l * attention_weights.squeeze(-1).unsqueeze(-1).unsqueeze(-1)  # B x C x H x W

        # Compute attention map by averaging over channels (B x H x W)
        attention_map = weighted_channels.mean(dim=1)  # B x H x W

        return attended_features, attention_map

import torch

# Adjust CNNEncoderWithAttention to accept 3-channel input (CIFAR-100)
class CNNEncoderWithAttentionAdjusted(CNNEncoderWithAttention):
    def __init__(self, num_classes):
        super().__init__(num_classes)
        self.fc2 = nn.Linear(16 * 16 * 256, 120)  # Adjust Matrivasha output size

# Initialize the models
pretrained_model = CustomCNN(num_classes=100)
target_model = CNNEncoderWithAttentionAdjusted(num_classes=120)

# Load the pre-trained weights
pretrained_weights_path = './cifar100_pretrained_weights.pth'
pretrained_model.load_state_dict(torch.load(pretrained_weights_path))

# Transfer weights
pretrained_state_dict = pretrained_model.state_dict()
target_state_dict = target_model.state_dict()

# Mapping weights from the pre-trained model to the target model
for name, param in pretrained_state_dict.items():
    if name in target_state_dict and param.size() == target_state_dict[name].size():
        target_state_dict[name].copy_(param)
        print(f"Transferred {name}.")

# Load the updated state dict into the target model
target_model.load_state_dict(target_state_dict)

# Save the updated target model
torch.save(target_model.state_dict(), './target_model_with_pretrained_weights.pth')
print("Weights transferred successfully.")

# Number of classes
num_classes = 120

# Initialize the model
model = CNNEncoderWithAttention(num_classes=num_classes)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

model = CNNEncoderWithAttentionAdjusted(num_classes=120)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

import torch
torch.cuda.empty_cache()

from tqdm import tqdm
import matplotlib.pyplot as plt

# Initialize lists to store losses
train_losses = []
val_losses = []

epochs = 2

# Training and validation loops
for epoch in range(epochs):
    # Training loop
    model.train()
    running_loss = 0.0
    train_loader_tqdm = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", unit="batch")

    for images, labels in train_loader_tqdm:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs, map1, map2, map3 = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        train_loader_tqdm.set_postfix(loss=(running_loss / len(train_loader)))

    train_loss = running_loss / len(train_loader)
    train_losses.append(train_loss)
    print(f"Epoch {epoch+1}, Training Loss: {train_loss:.4f}")

    # Validation loop
    model.eval()
    val_running_loss = 0.0
    val_loader_tqdm = tqdm(val_loader, desc="Validating", unit="batch")

    with torch.no_grad():
        for images, labels in val_loader_tqdm:
            images, labels = images.to(device), labels.to(device)
            outputs, map1, map2, map3 = model(images)
            loss = criterion(outputs, labels)
            val_running_loss += loss.item()

    val_loss = val_running_loss / len(val_loader)
    val_losses.append(val_loss)
    print(f"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}")

# Plot the training and validation losses



plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), train_losses, label="Training Loss")
plt.plot(range(1, epochs + 1), val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training and Validation Loss")
plt.legend()
plt.grid()
plt.show()

from tqdm import tqdm
import matplotlib.pyplot as plt

# Initialize lists to store losses
train_losses = []
val_losses = []

epochs = 2

# Training and validation loops
for epoch in range(epochs):
    # Training loop
    model.train()
    running_loss = 0.0
    train_loader_tqdm = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", unit="batch")

    for images, labels in train_loader_tqdm:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs, map1, map2, map3 = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        train_loader_tqdm.set_postfix(loss=(running_loss / len(train_loader)))

    train_loss = running_loss / len(train_loader)
    train_losses.append(train_loss)
    print(f"Epoch {epoch+1}, Training Loss: {train_loss:.4f}")

    # Validation loop
    model.eval()
    val_running_loss = 0.0
    val_loader_tqdm = tqdm(val_loader, desc="Validating", unit="batch")

    with torch.no_grad():
        for images, labels in val_loader_tqdm:
            images, labels = images.to(device), labels.to(device)
            outputs, map1, map2, map3 = model(images)
            loss = criterion(outputs, labels)
            val_running_loss += loss.item()

    val_loss = val_running_loss / len(val_loader)
    val_losses.append(val_loss)
    print(f"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}")

# Plot the training and validation losses



plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), train_losses, label="Training Loss")
plt.plot(range(1, epochs + 1), val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training and Validation Loss")
plt.legend()
plt.grid()
plt.show()

import seaborn as sns
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt

# Evaluate on test data
model.eval()  # Set the model to evaluation mode
all_preds = []
all_labels = []

with torch.no_grad():  # Disable gradient computation
    for images, labels in tqdm(test_loader, desc="Testing", unit="batch"):
        images, labels = images.to(device), labels.to(device)
        outputs, map1, map2, map3 = model(images)
        _, preds = torch.max(outputs, 1)  # Get the predicted class indices
        all_preds.extend(preds.cpu().numpy())  # Collect predictions
        all_labels.extend(labels.cpu().numpy())  # Collect ground truth labels

# Create the confusion matrix
cm = confusion_matrix(all_labels, all_preds)

# Convert to DataFrame for better readability with class labels
labels = range(len(cm))  # Replace with class names if available
cm_df = pd.DataFrame(cm, index=labels, columns=labels)

# Plot the confusion matrix using Seaborn
plt.figure(figsize=(12, 10))  # Set the figure size
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=True, linewidths=.5)
plt.title("Confusion Matrix", fontsize=16)
plt.ylabel("True Labels", fontsize=14)
plt.xlabel("Predicted Labels", fontsize=14)
plt.xticks(fontsize=12, rotation=45)
plt.yticks(fontsize=12)
plt.show()

# Print classification report
print(classification_report(all_labels, all_preds))

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

def create_heatmaps(input_images, map1, map2, map3, resize_size=128):

    batch_size = input_images.size(0)

    for b in range(batch_size):
        # Get a single input image
        img = input_images[b].squeeze().cpu().numpy()  # Shape: [H, W] (e.g., 128x128)

        # Normalize and resize map1
        heatmap1 = map1[b].unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, H1, W1]
        heatmap1 = F.interpolate(heatmap1, size=(resize_size, resize_size), mode="bilinear", align_corners=True).squeeze().cpu().numpy()
        heatmap1 = normalize_heatmap(heatmap1)

        # Normalize and resize map2
        heatmap2 = map2[b].unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, H2, W2]
        heatmap2 = F.interpolate(heatmap2, size=(resize_size, resize_size), mode="bilinear", align_corners=True).squeeze().cpu().numpy()
        heatmap2 = normalize_heatmap(heatmap2)

        # Normalize and resize map3
        heatmap3 = map3[b].unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, H3, W3]
        heatmap3 = F.interpolate(heatmap3, size=(resize_size, resize_size), mode="bilinear", align_corners=True).squeeze().cpu().numpy()
        heatmap3 = normalize_heatmap(heatmap3)

        # Plot original image and heatmaps
        fig, axes = plt.subplots(1, 4, figsize=(12, 3))

        # Original Image
        axes[0].imshow(img, cmap='gray')
        axes[0].set_title("Original Image")

        # Heatmap 1
        axes[1].imshow(img, cmap='gray')
        axes[1].imshow(heatmap1, cmap='jet', alpha=0.5)
        axes[1].set_title("Heatmap 1 (Resized)")

        # Heatmap 2
        axes[2].imshow(img, cmap='gray')
        axes[2].imshow(heatmap2, cmap='jet', alpha=0.5)
        axes[2].set_title("Heatmap 2 (Resized)")

        # Heatmap 3
        axes[3].imshow(img, cmap='gray')
        axes[3].imshow(heatmap3, cmap='jet', alpha=0.5)
        axes[3].set_title("Heatmap 3 (Resized)")

        for ax in axes:
            ax.axis('off')

        plt.tight_layout()
        plt.show()


def normalize_heatmap(heatmap):

    heatmap -= heatmap.min()
    if heatmap.max() > 0:
        heatmap /= heatmap.max()
    return heatmap

def visualize_predictions_with_heatmaps(model, test_loader, num_samples=30, correct=True, resize_size=128):
    """
    Visualizes correctly or incorrectly classified samples with heatmaps.

    Args:
        model (nn.Module): Trained model.
        test_loader (DataLoader): DataLoader for the test set.
        num_samples (int): Number of samples to visualize.
        correct (bool): Whether to visualize correct or incorrect predictions.
        resize_size (int): Target size to resize heatmaps.
    """
    model.eval()
    count = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs, map1, map2, map3 = model(inputs)
            prob = F.softmax(outputs, dim=1)
            preds = torch.argmax(outputs, dim=1)

            for i in range(inputs.size(0)):
                is_correct = preds[i] == labels[i]
                if is_correct == correct and count < num_samples:
                    # Generate heatmaps for the current sample
                    create_heatmaps(
                        input_images=inputs[i:i+1],  # Pass a single image
                        map1=map1[i:i+1],           # Corresponding map1
                        map2=map2[i:i+1],           # Corresponding map2
                        map3=map3[i:i+1],           # Corresponding map3
                        resize_size=resize_size
                    )

                    # Display prediction details
                    ground_truth = labels[i].item()
                    predicted = preds[i].item()
                    confidence = prob[i][preds[i]].item()
                    print(f"Ground Truth: {ground_truth}, Predicted: {predicted}, Probability: {confidence:.4f}")

                    count += 1
                    if count >= num_samples:
                        return

# Example Usage
print("Visualizing Correct Predictions:")
visualize_predictions_with_heatmaps(model, test_loader, num_samples=10, correct=True)

print("Visualizing Incorrect Predictions:")
visualize_predictions_with_heatmaps(model, test_loader, num_samples=10, correct=False)

def visualize_one_correct_and_one_incorrect_per_class(model, test_loader, num_classes=100, resize_size=128):
    """
    Visualizes one correct and one incorrect prediction for each class with heatmaps.

    Args:
        model (nn.Module): Trained model.
        test_loader (DataLoader): DataLoader for the test set.
        num_classes (int): Number of classes.
        resize_size (int): Target size to resize heatmaps.
    """
    model.eval()

    # Dictionaries to store one correct and one incorrect prediction per class
    correct_predictions = {}
    incorrect_predictions = {}

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs, map1, map2, map3 = model(inputs)
            prob = F.softmax(outputs, dim=1)
            preds = torch.argmax(outputs, dim=1)

            for i in range(inputs.size(0)):
                ground_truth = labels[i].item()
                predicted = preds[i].item()
                confidence = prob[i][predicted].item()

                if ground_truth not in correct_predictions and predicted == ground_truth:
                    # Store the first correct prediction for this class
                    correct_predictions[ground_truth] = {
                        "input": inputs[i:i+1],
                        "map1": map1[i:i+1],
                        "map2": map2[i:i+1],
                        "map3": map3[i:i+1],
                        "predicted": predicted,
                        "confidence": confidence,
                    }

                if ground_truth not in incorrect_predictions and predicted != ground_truth:
                    # Store the first incorrect prediction for this class
                    incorrect_predictions[ground_truth] = {
                        "input": inputs[i:i+1],
                        "map1": map1[i:i+1],
                        "map2": map2[i:i+1],
                        "map3": map3[i:i+1],
                        "predicted": predicted,
                        "confidence": confidence,
                    }

                # Stop if we have one correct and one incorrect for each class
                if len(correct_predictions) == num_classes and len(incorrect_predictions) == num_classes:
                    break

    # Visualize the collected predictions
    for class_id in range(num_classes):
        if class_id in correct_predictions:
            print(f"Class {class_id} - Correct Prediction:")
            correct = correct_predictions[class_id]
            create_heatmaps(
                input_images=correct["input"],
                map1=correct["map1"],
                map2=correct["map2"],
                map3=correct["map3"],
                resize_size=resize_size,
            )
            print(f"Ground Truth: {class_id}, Predicted: {correct['predicted']}, Probability: {correct['confidence']:.4f}")

        if class_id in incorrect_predictions:
            print(f"Class {class_id} - Incorrect Prediction:")
            incorrect = incorrect_predictions[class_id]
            create_heatmaps(
                input_images=incorrect["input"],
                map1=incorrect["map1"],
                map2=incorrect["map2"],
                map3=incorrect["map3"],
                resize_size=resize_size,
            )
            print(f"Ground Truth: {class_id}, Predicted: {incorrect['predicted']}, Probability: {incorrect['confidence']:.4f}")


# Example Usage
visualize_one_correct_and_one_incorrect_per_class(model, test_loader, num_classes=120)